<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.pica</groupId>
    <artifactId>bixiflink</artifactId>
    <packaging>pom</packaging>
    <version>1.0</version>

    <modules>
        <module>flink-learning-code</module>
        <module>khkw</module>
        <module>java-learning-code</module>
        <module>UserBehaviorAnalysis</module>
    </modules>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
        <scala.version>2.11.12</scala.version>
        <scala.binary.version>2.11</scala.binary.version>
        <flink.version>1.11.1</flink.version>
        <java.version>1.8</java.version>
        <parquet.version>1.10.1</parquet.version>
        <hadoop.version>2.7.3</hadoop.version>
        <hive.groupid>org.apache.hive</hive.groupid>
        <hive.version>2.3.1</hive.version>
        <hive.exec.classifier>core</hive.exec.classifier>
        <canal.client.version>1.1.4</canal.client.version>
        <flink_kafka_connector_version>flink-connector-kafka-0.10_2.11</flink_kafka_connector_version>
        <hudi.version>0.6.0</hudi.version>
        <mysql.version>5.1.6</mysql.version>
        <fastjson.version>1.2.51</fastjson.version>
        <log4j.version>1.2.17</log4j.version>
        <slf4j.version>1.7.7</slf4j.version>
        <flink.mysql.cdc.version>1.1.0</flink.mysql.cdc.version>
        <lombok.version>1.18.0</lombok.version>
        <junit.version>4.11</junit.version>
        <elasticsearch.version>6.8.12</elasticsearch.version>
        <project.build.scope>compile</project.build.scope>
    </properties>


    <dependencies>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>${scala.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>

        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
            <version>${slf4j.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>
        <dependency>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
            <version>${log4j.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>1.7.25</version>
        </dependency>

        <!--mysql JDBC-->
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>${mysql.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>

        <!-- flink的hadoop兼容 -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-hadoop-compatibility_2.11</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>

        <!-- flink的scala的api -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-scala_2.11</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>

        <!-- flink streaming的scala的api -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-scala_2.11</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>

        <!-- flink的java的api -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-java</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>

        <!-- flink streaming的java的api -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-java_2.11</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>

        <!-- flink 的kafka connector -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-kafka-0.10_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>


        <!-- 使用rocksdb保存flink的state -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-statebackend-rocksdb_2.11</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>

        <!-- flink运行时的webUI -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-runtime-web_2.11</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>

        <!--Flink 写文件到HDFS-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-filesystem_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>


        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-test-utils_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>


        <!--redis连接-->


        <!--Flink操作parquet格式-->
        <dependency>
            <groupId>org.apache.parquet</groupId>
            <artifactId>parquet-avro</artifactId>
            <version>${parquet.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.parquet</groupId>
            <artifactId>parquet-hadoop</artifactId>
            <version>${parquet.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-parquet_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>${hadoop.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-hdfs</artifactId>
            <version>${hadoop.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>


        <!--Flink Table如下配置-->
        <!-- 支持一些自定义的消息格式，比如kafka里面消息格式是json的，或者需要自定义函数支持 -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-common</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>
        <!-- 使用Blink Planner -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>
        <!--bridge 桥接器，主要负责 table API 和 DataStream/DataSet API的连接支持-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-java-bridge_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-scala-bridge_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>


        <!-- JDBC Connector的支持，本案例会是使用MySQL -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-jdbc_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>

        <!-- Kafka Connector的支持-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-sql-connector-kafka-0.10_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>

        <!-- Kafka里面的消息采用Json格式 -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-json</artifactId>
            <version>${flink.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>

        <!--Flink MYSQL CDC-->
        <dependency>
            <groupId>com.alibaba.ververica</groupId>
            <!-- add the dependency matching your database -->
            <artifactId>flink-connector-mysql-cdc</artifactId>
            <version>${flink.mysql.cdc.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>

        <!-- 辅助类增加get set toString等方法，需要idea安装lombok插件 -->
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <version>${lombok.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>

        <!--canal框架,解析binlog-->
        <dependency>
            <groupId>com.alibaba.otter</groupId>
            <artifactId>canal.client</artifactId>
            <version>${canal.client.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>

        <!--解析json-->
        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>${fastjson.version}</version>
            <scope>${project.build.scope}</scope>
        </dependency>

        <dependency>
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-lang3</artifactId>
            <version>3.5</version>
        </dependency>
    </dependencies>



    <dependencyManagement>
        <!--子模块需要手动引入,但是不需要加version-->
        <dependencies>
            <dependency>
                <groupId>org.apache.hudi</groupId>
                <artifactId>hudi-spark-bundle_2.11</artifactId>
                <version>${hudi.version}</version>
                <scope>${project.build.scope}</scope>
            </dependency>
            <dependency>
                <groupId>org.apache.hadoop</groupId>
                <artifactId>hadoop-client</artifactId>
                <version>${hadoop.version}</version>
                <scope>${project.build.scope}</scope>
            </dependency>
            <dependency>
                <groupId>org.elasticsearch</groupId>
                <artifactId>elasticsearch</artifactId>
                <version>${elasticsearch.version}</version>
            </dependency>
            <dependency>
                <groupId>org.elasticsearch.client</groupId>
                <artifactId>elasticsearch-rest-high-level-client</artifactId>
                <version>${elasticsearch.version}</version>
            </dependency>
        </dependencies>
    </dependencyManagement>





    <!--这时候在本地IDA运行Job时,它会给你引入 flink-java、flink-streaming-java,且 scope 设置为 compile,但是你是打成 Jar 包的时候它又不起作用。-->
<!--    <profiles>-->
<!--        <profile>-->
<!--            <id>add-dependencies-for-IDEA</id>-->
<!--            <activation>-->
<!--                <property>-->
<!--                    <name>idea.version</name>-->
<!--                </property>-->
<!--            </activation>-->

<!--            <dependencies>-->
<!--                <dependency>-->
<!--                    <groupId>org.apache.flink</groupId>-->
<!--                    <artifactId>flink-scala_2.11</artifactId>-->
<!--                    <version>${flink.version}</version>-->
<!--                    <scope>compile</scope>-->
<!--                </dependency>-->
<!--                <dependency>-->
<!--                    <groupId>org.apache.flink</groupId>-->
<!--                    <artifactId>flink-streaming-scala_2.11</artifactId>-->
<!--                    <version>${flink.version}</version>-->
<!--                    <scope>compile</scope>-->
<!--                </dependency>-->
<!--                <dependency>-->
<!--                    <groupId>org.apache.flink</groupId>-->
<!--                    <artifactId>flink-java</artifactId>-->
<!--                    <version>${flink.version}</version>-->
<!--                    <scope>compile</scope>-->
<!--                </dependency>-->
<!--                <dependency>-->
<!--                    <groupId>org.apache.flink</groupId>-->
<!--                    <artifactId>flink-streaming-java_2.11</artifactId>-->
<!--                    <version>${flink.version}</version>-->
<!--                    <scope>compile</scope>-->
<!--                </dependency>-->
<!--            </dependencies>-->
<!--        </profile>-->
<!--    </profiles>-->

</project>